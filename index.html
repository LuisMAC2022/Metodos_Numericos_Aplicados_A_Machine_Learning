<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Manual de Métodos Numéricos y Machine Learning</title>

  <!-- Integración de MathJax para renderizar LaTeX -->
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script> <!-- MathJax en GitHub Pages mediante CDN :contentReference[oaicite:7]{index=7} -->

  <!-- Hoja de estilos -->
  <link rel="stylesheet" href="css/styles.css" />
</head>
<body>
  <header>
    <h1>Hyper Red de Automatas Celulares Neuronales con logica diferencial.</h1>
    <nav>
      <ul>
        <li><a href="#introduccion">Introducción </a></li>
        <li><a href="#fundamentos">1. Fundamentos Matemáticos</a></li>
        <li><a href="#regresion">2. Modelos Clásicos de Regresión</a></li>
        <li><a href="#redes">3. Redes Neuronales</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <!-- Introducción -->
    <section id="introduccion">
      <h2>Introducción</h2>
      <p>
        Se esta desarrollando una revolucion que promete generar riqueza y desigualdad como nunca antes. Son pocos los paises(o empresas) en el mundo que tienen la capacidad de desarrollar modelos masivos de inteligencia artificial. 
        Este manual pretende ser una guía de iniciación al aprendizaje de maquinas (Machine Learning). Aprenderemos a generar sistemas que utilizar funciones matemáticas
      </p>
      <p>
        Cada sección incluye explicaciones teóricas, ejemplos y fórmulas clave, facilitando la comprensión y aplicación de los métodos en problemas reales.
      </p>
    </section>
    <!-- 1. Fundamentos Matemáticos -->
    <section id="fundamentos">
      <h2>1. Fundamentos Matemáticos y Métodos Auxiliares</h2>

      <!-- 1.1 Interpolación de Lagrange -->
      <article id="lagrange">
        <h3>1.1 Interpolación de Lagrange</h3>
        <p>
          La interpolación de Lagrange permite estimar el valor de una función desconocida utilizando puntos conocidos. Utiliza polinomios que pasan exactamente por los puntos dados, generando una fórmula explícita fácilmente aplicable.
        </p>
        <details>
          <summary class="summary-btn">Ver más detalles</summary>
          <div class="content">
            <p>
              Dados \(n+1\) puntos \((x_0,y_0),\dots,(x_n,y_n)\), el polinomio de Lagrange se define como:
            </p>
            <p>
              \[
                L(x) \;=\; \sum_{j=0}^{n} y_j \,\ell_j(x),
                \quad
                \ell_j(x)
                \;=\;
                \prod_{\substack{0 \le m \le n \\ m \neq j}}
                \frac{x - x_m}{x_j - x_m}.
              \]
            </p>
            <p>
              Este enfoque no requiere resolver un sistema de ecuaciones; en cambio, construye directamente el polinomio que interpola los puntos dados.
            </p>
          </div>
        </details> <!-- Uso de <details>–<summary> nativo para expandir contenido  -->
      </article>

      <!-- 1.2 Interpolación de Newton -->
      <article id="newton">
        <h3>1.2 Interpolación de Newton</h3>
        <p>
          El método de Newton facilita la interpolación mediante diferencias divididas, simplificando cálculos al añadir nuevos puntos sin necesidad de recalcular completamente el polinomio interpolador.
        </p>
        <details>
          <summary class="summary-btn">Ver fórmula y ejemplos</summary>
          <div class="content">
            <p>
              El polinomio de Newton se expresa como:
              \[
                P_n(x)
                \;=\;
                a_0
                \;+\;
                a_1(x - x_0)
                \;+\;
                a_2(x - x_0)(x - x_1)
                \;+\;
                \dots
                \;+\;
                a_n(x - x_0)\,\cdots\,(x - x_{n-1}),
              \]
              donde cada coeficiente \(a_k\) se calcula con diferencias divididas.
            </p>
            <p>
              La ventaja radica en que, si se agrega un punto extra \((x_{n+1},y_{n+1})\), solo se requiere calcular la última diferencia dividida y extender el polinomio sin rehacer todos los cálculos.
            </p>
          </div>
        </details>
      </article>

      <!-- 1.3 Extrapolación de datos -->
      <article id="extrapolacion">
        <h3>1.3 Extrapolación de datos</h3>
        <p>
          La extrapolación estima valores fuera del rango de datos conocidos usando modelos predictivos como regresión lineal simple y múltiple. Es importante que los datos presenten una relación lineal y se cuente con suficientes observaciones para obtener predicciones confiables.
        </p>
        <details>
          <summary class="summary-btn">Ver ejemplos de regresión</summary>
          <div class="content">
            <p>
              Para regresión lineal simple:
              \[
                \hat{y}
                \;=\;
                \beta_0
                \;+\;
                \beta_1\,x,
              \]
              y los coeficientes \(\beta_0,\beta_1\) se ajustan minimizando la suma de cuadrados de los residuos.
            </p>
            <p>
              Para regresión múltiple:
              \[
                \hat{y}
                \;=\;
                \beta_0
                \;+\;
                \beta_1 x_1
                + \dots
                + \beta_p x_p,
              \]
              donde se usa la matriz \(X\) de variables independientes y se calcula 
              \(\boldsymbol{\beta} = (X^T X)^{-1} X^T \mathbf{y}\).
            </p>
          </div>
        </details>
      </article>

      <!-- 1.4 Método del punto fijo -->
      <article id="punto-fijo">
        <h3>1.4 Método del punto fijo</h3>
        <p>
          Consiste en transformar una ecuación de la forma \(x = g(x)\), iterando sucesivamente hasta alcanzar una solución estable, llamada punto fijo. Es clave verificar que la función cumpla con condiciones de convergencia (por ejemplo, \(\lvert g'(x) \rvert < 1\)) para garantizar resultados válidos.
        </p>
        <details>
          <summary class="summary-btn">Ver pasos del algoritmo</summary>
          <div class="content">
            <ol>
              <li>Elige una aproximación inicial \(x^{(0)}\).</li>
              <li>Itera \(x^{(k+1)} = g\bigl(x^{(k)}\bigr)\) mientras \(\lvert x^{(k+1)} - x^{(k)}\rvert\) sea mayor que la tolerancia predefinida.</li>
              <li>Detén cuando la diferencia sea menor a la tolerancia deseada.</li>
            </ol>
          </div>
        </details>
      </article>

      <!-- 1.5 Método de Newton para sistemas no lineales -->
      <article id="newton-sistemas">
        <h3>1.5 Método de Newton para sistemas no lineales</h3>
        <p>
          El método de Newton permite resolver sistemas de ecuaciones no lineales mediante la linealización local del sistema alrededor de una aproximación inicial, actualizando iterativamente las soluciones hasta converger al resultado correcto.
        </p>
        <details>
          <summary class="summary-btn">Ver derivación e implementación</summary>
          <div class="content">
            <p>
              Para un sistema \(F(\mathbf{x}) = \mathbf{0}\), se define la actualización:
              \[
                \mathbf{x}^{(k+1)}
                \;=\;
                \mathbf{x}^{(k)}
                \;-\;
                J_F^{-1}\bigl(\mathbf{x}^{(k)}\bigr)\,F\bigl(\mathbf{x}^{(k)}\bigr),
              \]
              donde \(J_F\) es la matriz Jacobiana de \(F\). :contentReference[oaicite:9]{index=9}
            </p>
            <p>
              Cada iteración requiere resolver 
              \(J_F\bigl(\mathbf{x}^{(k)}\bigr)\,\Delta\mathbf{x}^{(k)} = -\,F\bigl(\mathbf{x}^{(k)}\bigr)\)
              para obtener \(\Delta\mathbf{x}^{(k)}\).
            </p>
          </div>
        </details>
      </article>
    </section>

    <!-- 2. Modelos Clásicos de Regresión -->
    <section id="regresion">
      <h2>2. Modelos Clásicos de Regresión</h2>

      <!-- 2.1 Regresión lineal simple -->
      <article id="regresion-lineal-simple">
        <h3>2.1 Regresión lineal simple: caso del consumo eléctrico</h3>
        <p>
          La regresión lineal simple modela una relación directa entre una variable independiente (por ejemplo, tiempo) y una dependiente (consumo eléctrico), permitiendo estimar valores futuros o analizar tendencias. :contentReference[oaicite:10]{index=10}
        </p>
        <details>
          <summary class="summary-btn">Ver fórmula y análisis de residuos</summary>
          <div class="content">
            <p>
              La fórmula es:
              \[
                \hat{y} = \beta_0 + \beta_1 x,
              \]
              donde \(\hat{y}\) es el consumo estimado, \(x\) el tiempo, y \(\beta_0\) y \(\beta_1\) los coeficientes a determinar.
            </p>
            <p>
              El análisis de residuos verifica la calidad del ajuste:
              \begin{itemize}
                <li>Residuo = Valor real - Valor ajustado.</li>
                <li>Graficar residuos ayuda a identificar patrones no capturados por el modelo.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>

      <!-- 2.2 Regresión lineal múltiple -->
      <article id="regresion-lineal-multiple">
        <h3>2.2 Regresión lineal múltiple: predicción de precios de vivienda</h3>
        <p>
          La regresión lineal múltiple extiende la anterior incluyendo más variables independientes, ajustándose a relaciones más complejas. Es crucial en econometría y análisis predictivo. :contentReference[oaicite:11]{index=11}
        </p>
        <details>
          <summary class="summary-btn">Ver fórmula y selección de variables</summary>
          <div class="content">
            <p>
              La fórmula general es:
              \[
                \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_n x_n,
              \]
              donde cada \(x_i\) es una variable independiente.
            </p>
            <p>
              La selección de variables es esencial:
              \begin{itemize}
                <li>Incluir solo variables relevantes para evitar sobreajuste.</li>
                <li>Usar criterios como AIC o BIC para evaluar modelos.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>

      <!-- 2.3 Regresión polinómica -->
      <article id="regresion-polinomica">
        <h3>2.3 Regresión polinómica: ajuste de curvas</h3>
        <p>
          La regresión polinómica ajusta un polinomio a los datos, siendo útil en relaciones no lineales. Se debe tener cuidado con el sobreajuste al usar polinomios de alto grado. :contentReference[oaicite:12]{index=12}
        </p>
        <details>
          <summary class="summary-btn">Ver fórmula y ejemplos prácticos</summary>
          <div class="content">
            <p>
              La fórmula es:
              \[
                \hat{y} = \beta_0 + \beta_1 x + \beta_2 x^2 + \dots + \beta_n x^n,
              \]
              donde los \(\beta_i\) son los coeficientes del polinomio.
            </p>
            <p>
              Ejemplo práctico:
              \begin{itemize}
                <li>Datos de ventas en función del tiempo.</li>
                <li>Un modelo cuadrático puede capturar tendencias mejor que uno lineal.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>

      <!-- 2.4 Regresión por mínimos cuadrados -->
      <article id="regresion-minimos-cuadrados">
        <h3>2.4 Regresión por mínimos cuadrados: fundamentos y geometría</h3>
        <p>
          Este método encuentra la línea que minimiza la suma de los cuadrados de las diferencias verticales entre los puntos de datos y la línea. Es fundamental en estadística y aprendizaje automático. :contentReference[oaicite:13]{index=13}
        </p>
        <details>
          <summary class="summary-btn">Ver derivación y propiedades</summary>
          <div class="content">
            <p>
              La línea de regresión se obtiene minimizando:
              \[
                \sum_{i=1}^{n} (y_i - \hat{y}_i)^2,
              \]
              donde \(y_i\) son los valores reales y \(\hat{y}_i\) los ajustados.
            </p>
            <p>
              Propiedades:
              \begin{itemize}
                <li>La línea pasa por el promedio de \(x\) y \(y\).</li>
                <li>Minimiza la varianza de los residuos.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>

      <!-- 2.5 Regresión ridge y lasso -->
      <article id="regresion-ridge-lasso">
        <h3>2.5 Regresión ridge y lasso: regularización de modelos</h3>
        <p>
          Estas técnicas añaden un término de penalización a la función de costo, ayudando a evitar el sobreajuste y mejorando la generalización del modelo. :contentReference[oaicite:14]{index=14}
        </p>
        <details>
          <summary class="summary-btn">Ver fórmulas y comparación</summary>
          <div class="content">
            <p>
              Ridge:
              \[
                \hat{\beta}^{ridge} = \arg\min_{\beta} \sum_{i=1}^{n} (y_i - x_i^T \beta)^2 + \lambda \lVert \beta \rVert_2^2,
              \]
            </p>
            <p>
              Lasso:
              \[
                \hat{\beta}^{lasso} = \arg\min_{\beta} \sum_{i=1}^{n} (y_i - x_i^T \beta)^2 + \lambda \lVert \beta \rVert_1,
              \]
            </p>
            <p>
              Comparativa:
              \begin{itemize}
                <li>Ridge utiliza norma \(L2\) y Lasso norma \(L1\).</li>
                <li>Lasso puede reducir algunos coeficientes a cero, actuando como un selector de variables.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>
    </section>

    <!-- 3. Redes Neuronales -->
    <section id="redes">
      <h2>3. Redes Neuronales</h2>

      <!-- 3.1 Perceptrón: la neurona artificial -->
      <article id="perceptron">
        <h3>3.1 Perceptrón: la neurona artificial</h3>
        <p>
          El perceptrón es la unidad básica de una red neuronal, simulando el funcionamiento de una neurona biológica. Aprende ajustando sus pesos sinápticos mediante el algoritmo de retropropagación del error. :contentReference[oaicite:15]{index=15}
        </p>
        <details>
          <summary class="summary-btn">Ver estructura y función</summary>
          <div class="content">
            <p>
              Estructura:
              \begin{itemize}
                <li>Entradas \(x_i\), cada una con un peso \(w_i\).</li>
                <li>Función de activación \(\phi\), típicamente una escalón o sigmoide.</li>
              </itemize}
            </p>
            <p>
              Función:
              \[
                y = \phi\biggl(\sum_{i} w_i x_i + b\biggr),
              \]
              donde \(b\) es el sesgo.
            </p>
          </div>
        </details>
      </article>

      <!-- 3.2 Arquitectura de una red neuronal -->
      <article id="arquitectura-red">
        <h3>3.2 Arquitectura de una red neuronal</h3>
        <p>
          Las redes neuronales constan de capas de neuronas: entrada, ocultas y salida. La profundidad y ancho de la red afectan su capacidad de aprendizaje y generalización. :contentReference[oaicite:16]{index=16}
        </p>
        <details>
          <summary class="summary-btn">Ver tipos de capas y funciones de activación</summary>
          <div class="content">
            <p>
              Tipos de capas:
              \begin{itemize}
                <li>Entrada: recibe las señales externas.</li>
                <li>Ocultas: procesan la información, pueden ser varias.</li>
                <li>Salida: entrega el resultado final.</li>
              </itemize}
            </p>
            <p>
              Funciones de activación:
              \begin{itemize}
                <li>Escalón, sigmoide, tangente hiperbólica, ReLU, entre otras.</li>
                <li>Eligen la salida de cada neurona basada en la suma ponderada de entradas.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>

      <!-- 3.3 Aprendizaje en redes neuronales -->
      <article id="aprendizaje-redes">
        <h3>3.3 Aprendizaje en redes neuronales</h3>
        <p>
          El aprendizaje consiste en ajustar los pesos y sesgos de la red minimizando el error entre la salida esperada y la obtenida. Se utiliza el algoritmo de retropropagación para actualizar los pesos en función del error. :contentReference[oaicite:17]{index=17}
        </p>
        <details>
          <summary class="summary-btn">Ver algoritmo de retropropagación</summary>
          <div class="content">
            <p>
              El algoritmo de retropropagación consta de los siguientes pasos:
              \begin{enumerate}
                <li>Propagación hacia adelante: calcular la salida de la red.</li>
                <li>Cálculo del error: comparar la salida con la esperada.</li>
                <li>Retropropagación del error: ajustar los pesos y sesgos.</li>
                <li>Repetir hasta minimizar el error.</li>
              </enumerate}
            </p>
          </div>
        </details>
      </article>

      <!-- 3.4 Redes neuronales convolucionales (CNN) -->
      <article id="redes-convolucionales">
        <h3>3.4 Redes neuronales convolucionales (CNN)</h3>
        <p>
          Las CNN son una clase de redes neuronales diseñadas para procesar datos con una estructura de cuadrícula, como imágenes. Utilizan capas convolucionales que aplican filtros a las entradas, detectando características locales. :contentReference[oaicite:18]{index=18}
        </p>
        <details>
          <summary class="summary-btn">Ver capas convolucionales y de agrupamiento</summary>
          <div class="content">
            <p>
              Capas convolucionales:
              \begin{itemize}
                <li>Aplican convoluciones a las entradas, extrayendo características.</li>
                <li>Usan filtros que se ajustan durante el entrenamiento.</li>
              </itemize}
            </p>
            <p>
              Capas de agrupamiento:
              \begin{itemize}
                <li>Reducen la dimensionalidad de las representaciones.</li>
                <li>Max pooling y average pooling son las más comunes.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>

      <!-- 3.5 Redes neuronales recurrentes (RNN) -->
      <article id="redes-recurrentes">
        <h3>3.5 Redes neuronales recurrentes (RNN)</h3>
        <p>
          Las RNN son adecuadas para datos secuenciales o temporales, ya que tienen conexiones recurrentes que les permiten mantener información en el tiempo. Son útiles en traducción automática, reconocimiento de voz, entre otros. :contentReference[oaicite:19]{index=19}
        </p>
        <details>
          <summary class="summary-btn">Ver arquitectura y aplicaciones</summary>
          <div class="content">
            <p>
              Arquitectura:
              \begin{itemize}
                <li>Neuronas con conexiones hacia adelante y hacia atrás.</li>
                <li>Estado oculto que captura información pasada.</li>
              </itemize}
            </p>
            <p>
              Aplicaciones:
              \begin{itemize}
                <li>Predicción de series temporales.</li>
                <li>Generación de texto.</li>
                <li>Modelado de lenguaje.</li>
              </itemize}
            </p>
          </div>
        </details>
      </article>
    </section>
  </main>

  <footer>
    <p>
      Manual de Métodos Numéricos y Machine Learning - Elaborado por [Su Nombre] - Fecha
    </p>
  </footer>
</body>
</html>
