<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Manual de Métodos Numéricos y Machine Learning</title>

  <!-- Integración de MathJax para renderizar LaTeX -->
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script> <!-- MathJax en GitHub Pages mediante CDN :contentReference[oaicite:7]{index=7} -->

  <!-- Hoja de estilos -->
  <link rel="stylesheet" href="css/styles.css" />
</head>
<body>
  <header>
    <h1>Manual de Métodos Numéricos y Machine Learning</h1>
    <nav>
      <ul>
        <li><a href="#fundamentos">1. Fundamentos Matemáticos</a></li>
        <li><a href="#regresion">2. Modelos Clásicos de Regresión</a></li>
        <li><a href="#redes">3. Redes Neuronales</a></li>
      </ul>
    </nav>
  </header>

  <main>
    <!-- 1. Fundamentos Matemáticos -->
    <section id="fundamentos">
      <h2>1. Fundamentos Matemáticos y Métodos Auxiliares</h2>

      <!-- 1.1 Interpolación de Lagrange -->
      <article id="lagrange">
        <h3>1.1 Interpolación de Lagrange</h3>
        <p>
          La interpolación de Lagrange permite estimar el valor de una función desconocida utilizando puntos conocidos. Utiliza polinomios que pasan exactamente por los puntos dados, generando una fórmula explícita fácilmente aplicable.
        </p>
        <details>
          <summary class="summary-btn">Ver más detalles</summary>
          <div class="content">
            <p>
              Dados \(n+1\) puntos \((x_0,y_0),\dots,(x_n,y_n)\), el polinomio de Lagrange se define como:
            </p>
            <p>
              \[
                L(x) \;=\; \sum_{j=0}^{n} y_j \,\ell_j(x),
                \quad
                \ell_j(x)
                \;=\;
                \prod_{\substack{0 \le m \le n \\ m \neq j}}
                \frac{x - x_m}{x_j - x_m}.
              \]
            </p>
            <p>
              Este enfoque no requiere resolver un sistema de ecuaciones; en cambio, construye directamente el polinomio que interpola los puntos dados.
            </p>
          </div>
        </details> <!-- Uso de <details>–<summary> nativo para expandir contenido  -->
      </article>

      <!-- 1.2 Interpolación de Newton -->
      <article id="newton">
        <h3>1.2 Interpolación de Newton</h3>
        <p>
          El método de Newton facilita la interpolación mediante diferencias divididas, simplificando cálculos al añadir nuevos puntos sin necesidad de recalcular completamente el polinomio interpolador.
        </p>
        <details>
          <summary class="summary-btn">Ver fórmula y ejemplos</summary>
          <div class="content">
            <p>
              El polinomio de Newton se expresa como:
              \[
                P_n(x)
                \;=\;
                a_0
                \;+\;
                a_1(x - x_0)
                \;+\;
                a_2(x - x_0)(x - x_1)
                \;+\;
                \dots
                \;+\;
                a_n(x - x_0)\,\cdots\,(x - x_{n-1}),
              \]
              donde cada coeficiente \(a_k\) se calcula con diferencias divididas.
            </p>
            <p>
              La ventaja radica en que, si se agrega un punto extra \((x_{n+1},y_{n+1})\), solo se requiere calcular la última diferencia dividida y extender el polinomio sin rehacer todos los cálculos.
            </p>
          </div>
        </details>
      </article>

      <!-- 1.3 Extrapolación de datos -->
      <article id="extrapolacion">
        <h3>1.3 Extrapolación de datos</h3>
        <p>
          La extrapolación estima valores fuera del rango de datos conocidos usando modelos predictivos como regresión lineal simple y múltiple. Es importante que los datos presenten una relación lineal y se cuente con suficientes observaciones para obtener predicciones confiables.
        </p>
        <details>
          <summary class="summary-btn">Ver ejemplos de regresión</summary>
          <div class="content">
            <p>
              Para regresión lineal simple:
              \[
                \hat{y}
                \;=\;
                \beta_0
                \;+\;
                \beta_1\,x,
              \]
              y los coeficientes \(\beta_0,\beta_1\) se ajustan minimizando la suma de cuadrados de los residuos.
            </p>
            <p>
              Para regresión múltiple:
              \[
                \hat{y}
                \;=\;
                \beta_0
                \;+\;
                \beta_1 x_1
                + \dots
                + \beta_p x_p,
              \]
              donde se usa la matriz \(X\) de variables independientes y se calcula 
              \(\boldsymbol{\beta} = (X^T X)^{-1} X^T \mathbf{y}\).
            </p>
          </div>
        </details>
      </article>

      <!-- 1.4 Método del punto fijo -->
      <article id="punto-fijo">
        <h3>1.4 Método del punto fijo</h3>
        <p>
          Consiste en transformar una ecuación de la forma \(x = g(x)\), iterando sucesivamente hasta alcanzar una solución estable, llamada punto fijo. Es clave verificar que la función cumpla con condiciones de convergencia (por ejemplo, \(\lvert g'(x) \rvert < 1\)) para garantizar resultados válidos.
        </p>
        <details>
          <summary class="summary-btn">Ver pasos del algoritmo</summary>
          <div class="content">
            <ol>
              <li>Elige una aproximación inicial \(x^{(0)}\).</li>
              <li>Itera \(x^{(k+1)} = g\bigl(x^{(k)}\bigr)\) mientras \(\lvert x^{(k+1)} - x^{(k)}\rvert\) sea mayor que la tolerancia predefinida.</li>
              <li>Detén cuando la diferencia sea menor a la tolerancia deseada.</li>
            </ol>
          </div>
        </details>
      </article>

      <!-- 1.5 Método de Newton para sistemas no lineales -->
      <article id="newton-sistemas">
        <h3>1.5 Método de Newton para sistemas no lineales</h3>
        <p>
          El método de Newton permite resolver sistemas de ecuaciones no lineales mediante la linealización local del sistema alrededor de una aproximación inicial, actualizando iterativamente las soluciones hasta converger al resultado correcto.
        </p>
        <details>
          <summary class="summary-btn">Ver derivación e implementación</summary>
          <div class="content">
            <p>
              Para un sistema \(F(\mathbf{x}) = \mathbf{0}\), se define la actualización:
              \[
                \mathbf{x}^{(k+1)}
                \;=\;
                \mathbf{x}^{(k)}
                \;-\;
                J_F^{-1}\bigl(\mathbf{x}^{(k)}\bigr)\,F\bigl(\mathbf{x}^{(k)}\bigr),
              \]
              donde \(J_F\) es la matriz Jacobiana de \(F\). :contentReference[oaicite:9]{index=9}
            </p>
            <p>
              Cada iteración requiere resolver 
              \(J_F\bigl(\mathbf{x}^{(k)}\bigr)\,\Delta\mathbf{x}^{(k)} = -\,F\bigl(\mathbf{x}^{(k)}\bigr)\)
              para obtener \(\Delta\mathbf{x}^{(k)}\).
            </p>
          </div>
        </details>
      </article>
    </section>

    <!-- 2. Modelos Clásicos de Regresión -->
    <section id="regresion">
      <h2>2. Modelos Clásicos de Regresión</h2>

      <!-- 2.1 Regresión lineal simple -->
      <article id="regresion-lineal-simple">
        <h3>2.1 Regresión lineal simple: caso del consumo eléctrico</h3>
        <p>
          La regresión lineal simple modela una relación directa entre una variable independiente (por ejemplo, tiempo) y una dependiente (consumo eléctrico), permitiendo estimar valores futuros o analizar tendencias. :contentReference[oaicite:10]{index=10}
        </p>
        <deta
